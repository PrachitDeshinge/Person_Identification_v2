{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab44a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Attempting to load pretrained weights ---\n",
      "Loading from: ../weights/transformer_120.pth\n",
      "\n",
      "[SUCCESS] Pretrained weights loaded successfully!\n",
      "\n",
      "--- Debugging Mismatched Keys ---\n",
      "\n",
      "Keys in the PRETRAINED FILE:\n",
      "base.cls_token\n",
      "base.pos_embed\n",
      "base.patch_embed.conv.0.weight\n",
      "base.patch_embed.conv.1.IN.weight\n",
      "base.patch_embed.conv.1.IN.bias\n",
      "base.patch_embed.conv.1.BN.weight\n",
      "base.patch_embed.conv.1.BN.bias\n",
      "base.patch_embed.conv.1.BN.running_mean\n",
      "base.patch_embed.conv.1.BN.running_var\n",
      "base.patch_embed.conv.1.BN.num_batches_tracked\n",
      "base.patch_embed.conv.3.weight\n",
      "base.patch_embed.conv.4.IN.weight\n",
      "base.patch_embed.conv.4.IN.bias\n",
      "base.patch_embed.conv.4.BN.weight\n",
      "base.patch_embed.conv.4.BN.bias\n",
      "base.patch_embed.conv.4.BN.running_mean\n",
      "base.patch_embed.conv.4.BN.running_var\n",
      "base.patch_embed.conv.4.BN.num_batches_tracked\n",
      "base.patch_embed.conv.6.weight\n",
      "base.patch_embed.conv.7.weight\n",
      "base.patch_embed.conv.7.bias\n",
      "base.patch_embed.conv.7.running_mean\n",
      "base.patch_embed.conv.7.running_var\n",
      "base.patch_embed.conv.7.num_batches_tracked\n",
      "base.patch_embed.proj.weight\n",
      "base.patch_embed.proj.bias\n",
      "base.blocks.0.norm1.weight\n",
      "base.blocks.0.norm1.bias\n",
      "base.blocks.0.attn.qkv.weight\n",
      "base.blocks.0.attn.qkv.bias\n",
      "base.blocks.0.attn.proj.weight\n",
      "base.blocks.0.attn.proj.bias\n",
      "base.blocks.0.norm2.weight\n",
      "base.blocks.0.norm2.bias\n",
      "base.blocks.0.mlp.fc1.weight\n",
      "base.blocks.0.mlp.fc1.bias\n",
      "base.blocks.0.mlp.fc2.weight\n",
      "base.blocks.0.mlp.fc2.bias\n",
      "base.blocks.1.norm1.weight\n",
      "base.blocks.1.norm1.bias\n",
      "base.blocks.1.attn.qkv.weight\n",
      "base.blocks.1.attn.qkv.bias\n",
      "base.blocks.1.attn.proj.weight\n",
      "base.blocks.1.attn.proj.bias\n",
      "base.blocks.1.norm2.weight\n",
      "base.blocks.1.norm2.bias\n",
      "base.blocks.1.mlp.fc1.weight\n",
      "base.blocks.1.mlp.fc1.bias\n",
      "base.blocks.1.mlp.fc2.weight\n",
      "base.blocks.1.mlp.fc2.bias\n",
      "base.blocks.2.norm1.weight\n",
      "base.blocks.2.norm1.bias\n",
      "base.blocks.2.attn.qkv.weight\n",
      "base.blocks.2.attn.qkv.bias\n",
      "base.blocks.2.attn.proj.weight\n",
      "base.blocks.2.attn.proj.bias\n",
      "base.blocks.2.norm2.weight\n",
      "base.blocks.2.norm2.bias\n",
      "base.blocks.2.mlp.fc1.weight\n",
      "base.blocks.2.mlp.fc1.bias\n",
      "base.blocks.2.mlp.fc2.weight\n",
      "base.blocks.2.mlp.fc2.bias\n",
      "base.blocks.3.norm1.weight\n",
      "base.blocks.3.norm1.bias\n",
      "base.blocks.3.attn.qkv.weight\n",
      "base.blocks.3.attn.qkv.bias\n",
      "base.blocks.3.attn.proj.weight\n",
      "base.blocks.3.attn.proj.bias\n",
      "base.blocks.3.norm2.weight\n",
      "base.blocks.3.norm2.bias\n",
      "base.blocks.3.mlp.fc1.weight\n",
      "base.blocks.3.mlp.fc1.bias\n",
      "base.blocks.3.mlp.fc2.weight\n",
      "base.blocks.3.mlp.fc2.bias\n",
      "base.blocks.4.norm1.weight\n",
      "base.blocks.4.norm1.bias\n",
      "base.blocks.4.attn.qkv.weight\n",
      "base.blocks.4.attn.qkv.bias\n",
      "base.blocks.4.attn.proj.weight\n",
      "base.blocks.4.attn.proj.bias\n",
      "base.blocks.4.norm2.weight\n",
      "base.blocks.4.norm2.bias\n",
      "base.blocks.4.mlp.fc1.weight\n",
      "base.blocks.4.mlp.fc1.bias\n",
      "base.blocks.4.mlp.fc2.weight\n",
      "base.blocks.4.mlp.fc2.bias\n",
      "base.blocks.5.norm1.weight\n",
      "base.blocks.5.norm1.bias\n",
      "base.blocks.5.attn.qkv.weight\n",
      "base.blocks.5.attn.qkv.bias\n",
      "base.blocks.5.attn.proj.weight\n",
      "base.blocks.5.attn.proj.bias\n",
      "base.blocks.5.norm2.weight\n",
      "base.blocks.5.norm2.bias\n",
      "base.blocks.5.mlp.fc1.weight\n",
      "base.blocks.5.mlp.fc1.bias\n",
      "base.blocks.5.mlp.fc2.weight\n",
      "base.blocks.5.mlp.fc2.bias\n",
      "base.blocks.6.norm1.weight\n",
      "base.blocks.6.norm1.bias\n",
      "base.blocks.6.attn.qkv.weight\n",
      "base.blocks.6.attn.qkv.bias\n",
      "base.blocks.6.attn.proj.weight\n",
      "base.blocks.6.attn.proj.bias\n",
      "base.blocks.6.norm2.weight\n",
      "base.blocks.6.norm2.bias\n",
      "base.blocks.6.mlp.fc1.weight\n",
      "base.blocks.6.mlp.fc1.bias\n",
      "base.blocks.6.mlp.fc2.weight\n",
      "base.blocks.6.mlp.fc2.bias\n",
      "base.blocks.7.norm1.weight\n",
      "base.blocks.7.norm1.bias\n",
      "base.blocks.7.attn.qkv.weight\n",
      "base.blocks.7.attn.qkv.bias\n",
      "base.blocks.7.attn.proj.weight\n",
      "base.blocks.7.attn.proj.bias\n",
      "base.blocks.7.norm2.weight\n",
      "base.blocks.7.norm2.bias\n",
      "base.blocks.7.mlp.fc1.weight\n",
      "base.blocks.7.mlp.fc1.bias\n",
      "base.blocks.7.mlp.fc2.weight\n",
      "base.blocks.7.mlp.fc2.bias\n",
      "base.blocks.8.norm1.weight\n",
      "base.blocks.8.norm1.bias\n",
      "base.blocks.8.attn.qkv.weight\n",
      "base.blocks.8.attn.qkv.bias\n",
      "base.blocks.8.attn.proj.weight\n",
      "base.blocks.8.attn.proj.bias\n",
      "base.blocks.8.norm2.weight\n",
      "base.blocks.8.norm2.bias\n",
      "base.blocks.8.mlp.fc1.weight\n",
      "base.blocks.8.mlp.fc1.bias\n",
      "base.blocks.8.mlp.fc2.weight\n",
      "base.blocks.8.mlp.fc2.bias\n",
      "base.blocks.9.norm1.weight\n",
      "base.blocks.9.norm1.bias\n",
      "base.blocks.9.attn.qkv.weight\n",
      "base.blocks.9.attn.qkv.bias\n",
      "base.blocks.9.attn.proj.weight\n",
      "base.blocks.9.attn.proj.bias\n",
      "base.blocks.9.norm2.weight\n",
      "base.blocks.9.norm2.bias\n",
      "base.blocks.9.mlp.fc1.weight\n",
      "base.blocks.9.mlp.fc1.bias\n",
      "base.blocks.9.mlp.fc2.weight\n",
      "base.blocks.9.mlp.fc2.bias\n",
      "base.blocks.10.norm1.weight\n",
      "base.blocks.10.norm1.bias\n",
      "base.blocks.10.attn.qkv.weight\n",
      "base.blocks.10.attn.qkv.bias\n",
      "base.blocks.10.attn.proj.weight\n",
      "base.blocks.10.attn.proj.bias\n",
      "base.blocks.10.norm2.weight\n",
      "base.blocks.10.norm2.bias\n",
      "base.blocks.10.mlp.fc1.weight\n",
      "base.blocks.10.mlp.fc1.bias\n",
      "base.blocks.10.mlp.fc2.weight\n",
      "base.blocks.10.mlp.fc2.bias\n",
      "base.blocks.11.norm1.weight\n",
      "base.blocks.11.norm1.bias\n",
      "base.blocks.11.attn.qkv.weight\n",
      "base.blocks.11.attn.qkv.bias\n",
      "base.blocks.11.attn.proj.weight\n",
      "base.blocks.11.attn.proj.bias\n",
      "base.blocks.11.norm2.weight\n",
      "base.blocks.11.norm2.bias\n",
      "base.blocks.11.mlp.fc1.weight\n",
      "base.blocks.11.mlp.fc1.bias\n",
      "base.blocks.11.mlp.fc2.weight\n",
      "base.blocks.11.mlp.fc2.bias\n",
      "base.norm.weight\n",
      "base.norm.bias\n",
      "base.fc.weight\n",
      "base.fc.bias\n",
      "classifier.weight\n",
      "bottleneck.weight\n",
      "bottleneck.bias\n",
      "bottleneck.running_mean\n",
      "bottleneck.running_var\n",
      "bottleneck.num_batches_tracked\n",
      "\n",
      "Keys in the MODEL ARCHITECTURE:\n",
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.conv.0.weight\n",
      "patch_embed.conv.1.IN.weight\n",
      "patch_embed.conv.1.IN.bias\n",
      "patch_embed.conv.1.BN.weight\n",
      "patch_embed.conv.1.BN.bias\n",
      "patch_embed.conv.1.BN.running_mean\n",
      "patch_embed.conv.1.BN.running_var\n",
      "patch_embed.conv.1.BN.num_batches_tracked\n",
      "patch_embed.conv.3.weight\n",
      "patch_embed.conv.4.IN.weight\n",
      "patch_embed.conv.4.IN.bias\n",
      "patch_embed.conv.4.BN.weight\n",
      "patch_embed.conv.4.BN.bias\n",
      "patch_embed.conv.4.BN.running_mean\n",
      "patch_embed.conv.4.BN.running_var\n",
      "patch_embed.conv.4.BN.num_batches_tracked\n",
      "patch_embed.conv.6.weight\n",
      "patch_embed.conv.7.weight\n",
      "patch_embed.conv.7.bias\n",
      "patch_embed.conv.7.running_mean\n",
      "patch_embed.conv.7.running_var\n",
      "patch_embed.conv.7.num_batches_tracked\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "fc.weight\n",
      "fc.bias\n",
      "\n",
      "--- End of Debugging ---\n",
      "\n",
      "Total trainable parameters in the loaded model: 237764544\n",
      "Total parameters in the model architecture: 237766467\n",
      "\n",
      "[WARNING] The number of parameters in the loaded model does not match the model architecture.\n",
      "Loaded parameters: 237764544, Architecture parameters: 237766467\n",
      "\n",
      "Model set to evaluation mode.\n",
      "\n",
      "[ERROR] An error occurred during the forward pass after loading weights: The size of tensor a (50) must match the size of tensor b (197) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from vit_model import VisionTransformer\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Change this path to where your downloaded model file is located.\n",
    "PRETRAINED_MODEL_PATH = '../weights/transformer_120.pth' \n",
    "\n",
    "# IMPORTANT: Adjust these parameters to match the model you downloaded.\n",
    "# These should match the configuration used to train the pretrained model.\n",
    "# If you don't know them, the defaults (e.g., 'base' ViT) are a good start.\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "EMBED_DIM = 768\n",
    "DEPTH = 12\n",
    "NUM_HEADS = 12\n",
    "NUM_CLASSES = 1000 # This might be different for a ReID model, but it's a starting point.\n",
    "\n",
    "# --- Model and Loading ---\n",
    "\n",
    "# 1. Instantiate the model with the correct architecture\n",
    "#    The model structure in code must match the structure of the saved weights.\n",
    "model = VisionTransformer(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    depth=DEPTH,\n",
    "    n_heads=NUM_HEADS,\n",
    "    n_classes=NUM_CLASSES,\n",
    ")\n",
    "\n",
    "print(\"--- Attempting to load pretrained weights ---\")\n",
    "print(f\"Loading from: {PRETRAINED_MODEL_PATH}\")\n",
    "\n",
    "try:\n",
    "    # 2. Load the downloaded weights\n",
    "    #    map_location='cpu' ensures the model loads even if you don't have a GPU\n",
    "    state_dict = torch.load(PRETRAINED_MODEL_PATH, map_location='cpu')\n",
    "    \n",
    "    # Often, pretrained models are saved inside a dictionary with a key like 'model' or 'state_dict'\n",
    "    # If the direct load fails, you might need to inspect the keys of the loaded dictionary.\n",
    "    # For example: if 'model' in state_dict: state_dict = state_dict['model']\n",
    "\n",
    "    # 3. Load the weights into the model\n",
    "    model.load_state_dict(state_dict, strict=False) # Use strict=False to be more lenient\n",
    "    \n",
    "    print(\"\\n[SUCCESS] Pretrained weights loaded successfully!\")\n",
    "\n",
    "    print(\"\\n--- Debugging Mismatched Keys ---\")\n",
    "    # Load the state dict again for inspection\n",
    "    checkpoint = torch.load(PRETRAINED_MODEL_PATH, map_location='cpu')\n",
    "\n",
    "    # It might be nested, check for common keys\n",
    "    if 'model' in checkpoint:\n",
    "        checkpoint = checkpoint['model']\n",
    "    if 'state_dict' in checkpoint:\n",
    "        checkpoint = checkpoint['state_dict']\n",
    "\n",
    "    print(\"\\nKeys in the PRETRAINED FILE:\")\n",
    "    for key in checkpoint.keys():\n",
    "        print(key)\n",
    "\n",
    "    print(\"\\nKeys in the MODEL ARCHITECTURE:\")\n",
    "    for key in model.state_dict().keys():\n",
    "        print(key)\n",
    "        \n",
    "    print(\"\\n--- End of Debugging ---\")\n",
    "\n",
    "    # Number of parameters in the loaded model and the model architecture\n",
    "    num_params_loaded = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal trainable parameters in the loaded model: {num_params_loaded}\")\n",
    "\n",
    "    # Number of parameters in the model architecture\n",
    "    num_params_architecture = sum(p.numel() for p in model.state_dict().values())\n",
    "    print(f\"Total parameters in the model architecture: {num_params_architecture}\")\n",
    "\n",
    "    if num_params_loaded != num_params_architecture:\n",
    "        print(\"\\n[WARNING] The number of parameters in the loaded model does not match the model architecture.\")\n",
    "        print(f\"Loaded parameters: {num_params_loaded}, Architecture parameters: {num_params_architecture}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to load pretrained weights. Reason: {e}\")\n",
    "    print(\"\\n--- Troubleshooting ---\")\n",
    "    print(\"This error usually means the model architecture in the code does not match the architecture in the saved file.\")\n",
    "    print(\"To debug, you can print the keys from both your model and the file to see how they differ.\")\n",
    "    # --- Code to add inside the 'except' block for debugging ---\n",
    "    \n",
    "# --- Verification ---\n",
    "\n",
    "# 4. Set the model to evaluation mode\n",
    "#    This is crucial for getting correct predictions (it disables layers like Dropout)\n",
    "model.eval()\n",
    "print(\"\\nModel set to evaluation mode.\")\n",
    "\n",
    "# 5. Perform a forward pass with a dummy image to ensure the model runs\n",
    "try:\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        dummy_image = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n",
    "        output = model(dummy_image)\n",
    "    \n",
    "    print(\"\\n--- Verification ---\")\n",
    "    print(\"[SUCCESS] Forward pass with loaded weights completed without errors.\")\n",
    "    print(f\"Input shape:  {dummy_image.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] An error occurred during the forward pass after loading weights: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd84efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Successfully read frame 110 from video ../v_0/input/3c.mp4.\n",
      "\n",
      "0: 384x640 3 persons, 170.6ms\n",
      "Speed: 7.6ms preprocess, 170.6ms inference, 11.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[SUCCESS] Detected 3 persons in the frame.\n",
      "[INFO] Cropping person 0 with bounding box: [[     807.12      222.78      895.39      486.35]]\n",
      "[SUCCESS] Saved cropped person 0 to 'person_crop_0.jpg'.\n",
      "[INFO] Cropping person 1 with bounding box: [[     741.28      118.91      792.43      261.58]]\n",
      "[SUCCESS] Saved cropped person 1 to 'person_crop_1.jpg'.\n",
      "[INFO] Cropping person 2 with bounding box: [[     788.63         112      837.43      254.65]]\n",
      "[SUCCESS] Saved cropped person 2 to 'person_crop_2.jpg'.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "video_file = '../v_0/input/3c.mp4'\n",
    "\n",
    "# Take frame number 100 from the video\n",
    "cap = cv.VideoCapture(video_file)\n",
    "frame_number = 110\n",
    "cap.set(cv.CAP_PROP_POS_FRAMES, frame_number)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "if not ret:\n",
    "    print(f\"[ERROR] Could not read frame {frame_number} from video {video_file}.\")\n",
    "else:\n",
    "    print(f\"[SUCCESS] Successfully read frame {frame_number} from video {video_file}.\")\n",
    "\n",
    "    # Using Yolo detect the person and save the crops\n",
    "    model = YOLO('../weights/yolo11m.pt')  # Load the YOLO model\n",
    "    results = model(frame)  # Perform inference on the frame\n",
    "\n",
    "    # Extract the bounding boxes for persons\n",
    "    person_boxes = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            if box.cls == 0:  # Class 0 is typically 'person' in YOLO\n",
    "                person_boxes.append(box.xyxy.cpu().numpy())\n",
    "    if not person_boxes:\n",
    "        print(\"[WARNING] No persons detected in the frame.\")\n",
    "    else:\n",
    "        print(f\"[SUCCESS] Detected {len(person_boxes)} persons in the frame.\")\n",
    "\n",
    "    # Crop and save the detected persons\n",
    "    for i, box in enumerate(person_boxes):\n",
    "        print(f\"[INFO] Cropping person {i} with bounding box: {box}\")\n",
    "        x1, y1, x2, y2 = map(int, box.flatten())\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "        cv.imwrite(f'person_crop_{i}.jpg', person_crop)\n",
    "        print(f\"[SUCCESS] Saved cropped person {i} to 'person_crop_{i}.jpg'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe8fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
